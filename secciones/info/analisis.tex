\documentclass[../../main.tex]{subfiles}

\begin{document}

Se parte de un conjunto de datos con un número de registros entre 1000 y 10000.  \\

\begin{lstlisting}[language=R]
> data <- db.fetch_all("SELECT post.*, subreddit.name as subreddit, user.name as username 
					  FROM post JOIN subreddit ON post.subreddit_id = subreddit.id JOIN user ON post.user_id = user.id  
					  WHERE date(post.created) >= '2021-01-14' AND date(post.created) <= '2021-09-12' 
					  ORDER BY post.created DESC LIMIT 10000 ;")
\end{lstlisting}

\vskip 0.2in

Normalizamos los datos usando las siguientes funciones:

\begin{lstlisting}[language=R]
> data_df <- data %>% select(upvote_ratio, total_awards_received, score, domain, subreddit)
> data_df[sapply(data_df, is.integer)] <- lapply(data_df[sapply(data_df, is.integer)], as.numeric)
> data_df[sapply(data_df, is.character)] <- lapply(data_df[sapply(data_df, is.character)], as.factor)
> data_df[is.na(data_df)] <- 0
> data_df <- discretizeDF(data_df, default = list(method = "interval", breaks = 3, labels = c("Low", "Medium", "High")))

\end{lstlisting}

\vskip 0.2in

Y creamos una matriz binaria en base a estos datos:

\begin{lstlisting}[language=R]
> create_binary_matrix <- function(cols, data){
    length_cols <- length(colnames(data))
    length_rows <- length(rownames(data))
    output <- data.frame(matrix(ncol = length(cols), nrow = 0))
    colnames(output) <- cols
    
    for(i in 1:length_rows){
        for(k in 1:length_cols){
            colname <- paste0(colnames(data)[k], "-", data[i, k])
            other <- paste0(colnames(data)[k], "-other")
            if(colname %in% cols){
                output[i, colname] <- 1
            } else{
                output[i, other] <- 1
            }
        }
    }
    output[is.na(output)] <- 0
    output[sapply(output, is.numeric)] <- lapply(output[sapply(output, is.numeric)], as.factor)
    return(output)
}

> top_n_domains <- data_df %>%
	select(domain) %>%
	group_by(domain) %>%
	summarise(n = n()) %>%
	filter(n > quantile(n, SHINY_FREQ_PERCENTAGE)) %>%
	add_row(domain = "other", n=0) %>%
	mutate_if(is.character,
			  str_replace_all, pattern = "self", replacement = "subreddit")

> top_n_subreddits <- data_df %>%
	select(subreddit) %>%
	group_by(subreddit) %>%
	summarise(n = n()) %>%
	filter(n > quantile(n, SHINY_FREQ_PERCENTAGE)) %>%
	add_row(subreddit = "other", n = 0)

> cols <- c(paste("upvote_ratio", names(table(data_df$upvote_ratio)), sep = "-"),
		  paste("total_awards_received", names(table(data_df$total_awards_received)), sep = "-"),
		  paste("score", names(table(data_df$score)), sep = "-"),
		  paste("domain", top_n_domains$domain, sep = "-"),
		  paste("subreddit", top_n_subreddits$subreddit, sep = "-"))

> data_matrix <- create_binary_matrix(cols, data_df); data_matrix[1:5, 1:5]

\end{lstlisting}

\vskip 0.2in

\begin{table}[H]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{|l|l|l|l|l|l|}
\hline
 &
  \textbf{upvote\_ratio-Low} &
  \textbf{upvote\_ratio-Medium} &
  \textbf{upvote\_ratio-High} &
  \textbf{total\_awards\_received-Low} &
  \textbf{total\_awards\_received-Medium} \\ \hline
1 & 0 & 1 & 0 & 1 & 0 \\ \hline
2 & 0 & 0 & 1 & 1 & 0 \\ \hline
3 & 0 & 0 & 1 & 1 & 0 \\ \hline
4 & 0 & 0 & 1 & 1 & 0 \\ \hline
5 & 0 & 0 & 1 & 1 & 0 \\ \hline
\end{tabular}%
}
\end{table}

Dado que crear un contexto formal con tantas filas consume muchos recursos de memoria del servidor o equipo, es necesario buscar una alternativa para poder reducir esta carga. Se precisa buscar otra solución en la cual se puede trabajar con un número razonable de reglas sin un exceso en el consumo de memoria. \\

La solución propuesta es crear un objeto de tipo \textit{transactions} de la matriz binaria que se ha creado en el punto anterior.

\begin{lstlisting}[language=R]
> transactions <- as(data_matrix, "transactions")
\end{lstlisting}

\vskip 0.2in

Se aplica el algoritmo \gls{apriori} con unos parametros $support = 0.5$ y $confidence = 1$ y con esto conseguimos un número determinado de reglas:

\begin{lstlisting}[language=R]
> rules <- apriori(transactions, parameter = list(support = 0.5, confidence = 1, maxlen = 5));
Apriori

Parameter specification:
 confidence minval smax arem  aval originalSupport maxtime support minlen maxlen target  ext
          1    0.1    1 none FALSE            TRUE       5     0.5      1      5  rules TRUE

Algorithmic control:
 filter tree heap memopt load sort verbose
    0.1 TRUE TRUE  FALSE TRUE    2    TRUE

Absolute minimum support count: 5000 

set item appearances ...[0 item(s)] done [0.00s].
set transactions ...[265 item(s), 10000 transaction(s)] done [0.07s].
sorting and recoding items ... [137 item(s)] done [0.02s].
creating transaction tree ... done [0.01s].
checking subsets of size 1 2 3 4 done [39.08s].
writing ... [3957333 rule(s)] done [1.26s].
creating S4 object  ... done [1.33s].
\end{lstlisting}

\vskip 0.2in

Una vez ejecutado el algoritmo \textit{\Gls{apriori}} para reducir el número de reglas podemos trabajar con las reglas redundantes o las reglas significantes:

\begin{lstlisting}[language=R]
> rules.non_redundant <- rules[!is.redundant(rules)]; rules.non_redundant
set of 50 rules 

> rules.significant <- rules[is.significant(rules, transactions)]; 
set of 82851 rules 
\end{lstlisting}

\vskip 0.2in

En este caso vemos que obtenemos muy pocos reglas no redundantes y por tanto se trabajaría con estas reglas. Ahora el siguiente paso es convertir estas reglas a una matriz dispersa y posteriormente convertir esta matriz en un objeto de tipo \textit{transactions}

\begin{lstlisting}[language=R]
> transactions_reduced <- transactions(as(items(rules.non_redundant), "ngCMatrix")); transactions_reduced
transactions in sparse format with
 50 transactions (rows) and
 265 items (columns)
\end{lstlisting}

\vskip 0.2in

Una vez hecho procedemos a crear un contexto formal con estas transacciones ya reducidas.

\begin{lstlisting}[language=R]
> fc <- FormalContext$new(transactions_reduced)
\end{lstlisting}

\vskip 0.2in

Una vez creado el contexto procedemos al análisis de la información del conjunto de datos.

Primero, es posible calcular los conceptos de este conjunto de datos, la cual hace uso del algoritmo difuso \textit{NextClosure} \cite{nextclosure}:

\begin{lstlisting}[language=R]
> fc$find_concepts()

> fc$concepts$size()
[1] 67

> head(fc$concepts)
A set of 6 concepts:
1: ({1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50}, {})
2: ({14, 15, 16, 17, 33, 34, 35, 36, 37, 47, 48, 49}, {subreddit-other=0})
3: ({33, 34, 40, 41, 42, 43, 44, 45, 47, 48, 50}, {subreddit-worldnews=0})
4: ({33, 34, 47, 48}, {subreddit-worldnews=0, subreddit-other=0})
5: ({49, 50}, {subreddit-todayilearned=0})
6: ({40, 41, 42, 43}, {subreddit-news=0, subreddit-worldnews=0})
\end{lstlisting}

\vskip 0.2in
			
Una vez hecho podemos calcular cierres, primero lo realizamos con el \textit{\gls{extent}} que se trata del conjunto con los objetos.

\begin{lstlisting}[language=R]
> S <- Set$new(attributes = fc$objects)
> S$assign(attributes = "1", values = 1); S
{1}

> fc$intent(S)
{domain-subreddit.unpopularopinion=0}
\end{lstlisting}

\vskip 0.2in

Después realizamos lo mismo pero esta vez con el \textit{\gls{intent}} que se trata del conjunto con los atributos:

\begin{lstlisting}[language=R]
> S <- Set$new(attributes = fc$attributes)
> S$assign(attributes = "subreddit-worldnews=0", values = 1); S
{subreddit-worldnews=0}

> fc$extent(S)
{33, 34, 40, 41, 42, 43, 44, 45, 47, 48, 50}
\end{lstlisting}

\vskip 0.2in

Es posible seleccionar un sub-grafo en la cual se cumple que el soporte sea mayor que un valor, en este caso 0.2.

\begin{lstlisting}[language=R]

> fc$concepts$support()
 [1] 1.00 0.24 0.22 0.08 0.04 0.08 0.04 0.10 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.08 0.02 0.02 0.06
[32] 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.08 0.10 0.04 0.04 0.02 0.12 0.02 0.02 0.02 0.02 0.02 0.12 0.02 0.02 0.02 0.02 0.04 0.10 0.02 0.08 0.02 0.06 0.02
[63] 0.06 0.02 0.04 0.02 0.00

> idx <- which(fc$concepts$support() > 0.2)
> sublattice <- fc$concepts$sublattice(idx); sublattice
A set of 4 concepts:
1: ({1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50}, {})
2: ({14, 15, 16, 17, 33, 34, 35, 36, 37, 47, 48, 49}, {subreddit-other=0})
3: ({33, 34, 40, 41, 42, 43, 44, 45, 47, 48, 50}, {subreddit-worldnews=0})
4: ({33, 34, 47, 48}, {subreddit-worldnews=0, subreddit-other=0})
\end{lstlisting}

\vskip 0.2in

También en base a un concepto en concreto podemos obtener más información correspondiente al grafo como pueden ser los infimos, supremos, superconceptos, subconceptos.

\begin{lstlisting}[language=R]
> concept <- fc$concepts[31]; concept
A set of 1 concepts:
1: ({40, 44, 45}, {domain-france24.com=0, subreddit-worldnews=0})

> fc$concepts$superconcepts(concept)
A set of 3 concepts:
1: ({1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50}, {})
2: ({33, 34, 40, 41, 42, 43, 44, 45, 47, 48, 50}, {subreddit-worldnews=0})
3: ({40, 44, 45}, {domain-france24.com=0, subreddit-worldnews=0})

> fc$concepts$subconcepts(concept)
A set of 5 concepts:
1: ({40, 44, 45}, {domain-france24.com=0, subreddit-worldnews=0})
2: ({40}, {domain-france24.com=0, subreddit-news=0, subreddit-worldnews=0})
3: ({44}, {upvote_ratio-High=1, domain-france24.com=0, subreddit-worldnews=0})
4: ({45}, {upvote_ratio-Medium=0, domain-france24.com=0, subreddit-worldnews=0})
5: ({}, {upvote_ratio-Low=0, upvote_ratio-Low=1, upvote_ratio-Medium=0, upvote_ratio-Medium=1, ....})

> concepts <- fc$concepts[1:3]

> fc$concepts$supremum(concepts)
({1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42,
  43, 44, 45, 46, 47, 48, 49, 50}, {})
  
> fc$concepts$infimum(concepts)
({33, 34, 47, 48}, {subreddit-worldnews=0, subreddit-other=0})

> head(fc$concepts$join_irreducibles())
A set of 6 concepts:
1: ({21}, {domain-other=0, subreddit-lifeprotips=0})
2: ({18}, {domain-other=0, subreddit-askreddit=0})
3: ({19}, {domain-other=0, subreddit-abrathatfits=0})
4: ({20}, {domain-other=0, subreddit-3amjokes=0})
5: ({36}, {domain-youtu.be=0, subreddit-ableton=0, subreddit-other=0})
6: ({22}, {domain-v.redd.it=0, subreddit-publicfreakout=0})
\end{lstlisting}

\vskip 0.2in

Es posible reducir el contexto formal, permitiendo que se puedan obtener menos conceptos o implicaciones. 

Antes de reducir el contexto formal tenemos los siguientes tamaños:

\begin{lstlisting}[language=R]
> fc$find_concepts()

> fc$concepts$size()
[1] 67

> fc$find_implications()

> fc$implications$cardinality()
[1] 527
\end{lstlisting}

\vskip 0.2in

Después de haber reducido el contexto formal se observa que se obtienen menos implicaciones

\begin{lstlisting}[language=R]
> fc$reduce()

> fc$find_concepts()

> fc$concepts$size()
[1] 67

> fc$find_implications()

> fc$implications$cardinality()
[1] 300
\end{lstlisting}

\vskip 0.2in

Haciendo uso del algoritmo \textit{NextClosure} \cite{nextclosure} es posible obtener implicaciones tal como se utiliza para computar los conceptos:

\begin{lstlisting}[language=R]
> head(fc$implications)
Implication set with 6 implications.
Rule 1: {[domain-youtu.be=0, subreddit-ableton=0]} -> {subreddit-other=0}
Rule 2: {[domain-theguardian.com=0, subreddit-science=0]} -> {domain-other=0}
Rule 3: {[domain-bloomberg.com=0, subreddit-technology=0]} -> {subreddit-worldnews=0, subreddit-other=0}
Rule 4: {subreddit-other=0, [domain-v.redd.it=0, subreddit-publicfreakout=0]} -> subreddit-lifeprotips=0, subreddit-news=0, subreddit-todayilearned=0, subreddit-worldnews=0, [domain-bloomberg.com=0, subreddit-technology=0], [domain-theguardian.com=0, subreddit-science=0], [domain-youtu.be=0, subreddit-ableton=0], ...}
Rule 5: {subreddit-worldnews=0, [domain-v.redd.it=0, subreddit-publicfreakout=0]} -> {upvote_ratio-Low=0, upvote_ratio-Medium=0, upvote_ratio-High=1, total_awards_received-Low=1, total_awards_received-Medium=0, total_awards_received-High=0, score-Low=1, score-Medium=0, score-High=0, domain-abc.net.au=0,
  domain-aljazeera.com=0, domain-cbc.ca=0, ...}
Rule 6: {subreddit-worldnews=0, subreddit-other=0, [domain-youtu.be=0, subreddit-ableton=0]} -> subreddit-
  leopardsatemyface=0, subreddit-lifeprotips=0, subreddit-news=0, subreddit-todayilearned=0, [domain-bloomberg.com=0, subreddit-technology=0], [domain-theguardian.com=0, subreddit-science=0], [domain-v.redd.it=0, subreddit-publicfreakout=0], ...}

\end{lstlisting}

\vskip 0.2in

También es posible eliminar redundancia aplicando técnicas de simplificación lógica:

\begin{lstlisting}[language=R]
> fc$implications$apply_rules(c("reduction", "composition", "generalization", "simplification"))
Processing batch

--> Reduction: from 300 to 300 in 0 secs.
--> Composition: from 300 to 300 in 0 secs.
--> Generalization: from 300 to 300 in 0 secs.
--> Simplification: from 300 to 300 in 0.14 secs.
Batch took 0.14 secs. 
\end{lstlisting}

\vskip 0.2in

Por último, es posible exportar las implicaciones computadas de un contexto formal a reglas de asociación:

\begin{lstlisting}[language=R]
> rules <- fc$implications$to_arules(); rules
set of 300 rules 
\end{lstlisting}

\vskip 0.2in

Con esto podemos ordenar por los distintos parámetros disponibles bajo el paquete \textit{arules}, por ejemplo en este caso ordenamos por los parámetros \textit{lift} y \textit{support}:

\begin{lstlisting}[language=R]
> lift <- arules::sort(rules, by="lift")
> support <- arules::sort(rules, by="support")

> lift.df <- data.frame(inspect(head(lift, 10)))[c('lhs', 'rhs', 'support', 'confidence', 'lift')]; lift.df
\end{lstlisting}

\vskip 0.2in

\begin{table}[H]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{|l|l|l|l|l|l|}
\hline
\textbf{} & \textbf{lhs}                                     & \textbf{rhs}                                & \textbf{support} & \textbf{confidence} & \textbf{lift} \\ \hline
1 & \{domain-twitter.com=0\}                                 & \{subreddit-leopardsatemyface=0,subreddit-other=0\} & 0.02040816 & 1 & 24.50000 \\ \hline
2         & \{upvote\_ratio-High=1,subreddit-worldnews=0\}   & \{domain-france24.com=0\}                   & 0.02040816       & 1                   & 16.33333      \\ \hline
3         & \{upvote\_ratio-Medium=0,subreddit-worldnews=0\} & \{domain-france24.com=0                     & 0.02040816       & 1                   & 16.33333      \\ \hline
4 & \{{[}domain-bloomberg.com=0, subreddit-technology=0{]}\} & \{subreddit-worldnews=0,subreddit-other=0           & 0.02040816 & 1 & 12.25000 \\ \hline
5         & \{subreddit-cursedcomments=0\}                   & \{domain-i.redd.it=0\}                      & 0.02040816       & 1                   & 12.25000      \\ \hline
6         & \{subreddit-antiwork=0\}                         & \{domain-i.redd.it=0\}                      & 0.02040816       & 1                   & 12.25000      \\ \hline
7         & \{domain-euronews.com=0\}                        & \{subreddit-worldnews=0,subreddit-other=0\} & 0.02040816       & 1                   & 12.25000      \\ \hline
8         & \{domain-dailymail.co.uk=0\}                     & \{subreddit-worldnews=0,subreddit-other=0\} & 0.02040816       & 1                   & 12.25000      \\ \hline
9         & \{domain-cbc.ca=0\}                              & \{subreddit-news=0,subreddit-worldnews=0\}  & 0.02040816       & 1                   & 12.25000      \\ \hline
10        & \{domain-aljazeera.com=0\}                       & \{subreddit-news=0,subreddit-worldnews=0\}  & 0.02040816       & 1                   & 12.25000      \\ \hline
\end{tabular}%
}
\end{table}

\begin{lstlisting}[language=R]
> support.df <- data.frame(inspect(head(support, 10)))[c('lhs', 'rhs', 'support', 'confidence', 'lift')]; support.df
\end{lstlisting}

\vskip 0.2in

\begin{table}[H]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{|l|l|l|l|l|l|}
\hline
\textbf{} & \textbf{lhs}                      & \textbf{rhs}              & \textbf{support} & \textbf{confidence} & \textbf{lift} \\ \hline
1         & \{subreddit-news=0\}              & \{subreddit-worldnews=0\} & 0.02040816       & 1                   & 4.454545      \\ \hline
2         & \{domain-france24.com=0\}         & \{subreddit-worldnews=0\} & 0.02040816       & 1                   & 4.454545      \\ \hline
3         & \{subreddit-leopardsatemyface=0\} & \{subreddit-other=0\}     & 0.02040816       & 1                   & 4.083333      \\ \hline
4         & \{score-High=0\}                  & \{score-Low=1\}           & 0.02040816       & 1                   & 9.800000      \\ \hline
5 & \{{[}domain-youtu.be=0, subreddit-ableton=0{]}\}         & \{subreddit-other=0\}                       & 0.02040816 & 1 & 4.083333 \\ \hline
6 & \{{[}domain-theguardian.com=0, subreddit-science=0{]}\}  & \{domain-other=0\}                          & 0.02040816 & 1 & 9.800000 \\ \hline
7 & \{{[}domain-bloomberg.com=0, subreddit-technology=0{]}\} & \{subreddit-worldnews=0,subreddit-other=0\} & 0.02040816 & 1 & 12.25000 \\ \hline
8         & \{subreddit-lifeprotips=0\}       & \{domain-other=0\}        & 0.02040816       & 1                   & 9.800000      \\ \hline
9         & \{subreddit-cursedcomments=0\}    & \{domain-i.redd.it=0\}    & 0.02040816       & 1                   & 12.25000      \\ \hline
10        & \{subreddit-askreddit=0\}         & \{domain-other=0\}        & 0.02040816       & 1                   & 9.800000      \\ \hline
\end{tabular}%
}
\end{table}

\end{document}